{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acad626-fca5-4192-b941-c57744ee7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn to draw gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2dbfbc-95a6-4c6b-a2d8-04329350e5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision.ops import nms, box_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09bae1f3-3590-4f37-a723-7c23db40c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "75dcb36c-7455-4367-81ae-a2461dfa1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_radius(det_size, min_overlap=0.7):\n",
    "  height, width = det_size\n",
    "\n",
    "  a1  = 1\n",
    "  b1  = (height + width)\n",
    "  c1  = width * height * (1 - min_overlap) / (1 + min_overlap)\n",
    "  sq1 = np.sqrt(b1 ** 2 - 4 * a1 * c1)\n",
    "  r1  = (b1 + sq1) / 2\n",
    "\n",
    "  a2  = 4\n",
    "  b2  = 2 * (height + width)\n",
    "  c2  = (1 - min_overlap) * width * height\n",
    "  sq2 = np.sqrt(b2 ** 2 - 4 * a2 * c2)\n",
    "  r2  = (b2 + sq2) / 2\n",
    "\n",
    "  a3  = 4 * min_overlap\n",
    "  b3  = -2 * min_overlap * (height + width)\n",
    "  c3  = (min_overlap - 1) * width * height\n",
    "  sq3 = np.sqrt(b3 ** 2 - 4 * a3 * c3)\n",
    "  r3  = (b3 + sq3) / 2\n",
    "  return min(r1, r2, r3)\n",
    "\n",
    "def gaussian_radius2(det_size, min_overlap):\n",
    "    height, width = det_size\n",
    "\n",
    "    a1  = 1\n",
    "    b1  = (height + width)\n",
    "    c1  = width * height * (1 - min_overlap) / (1 + min_overlap)\n",
    "    sq1 = np.sqrt(b1 ** 2 - 4 * a1 * c1)\n",
    "    r1  = (b1 - sq1) / (2 * a1)\n",
    "\n",
    "    a2  = 4\n",
    "    b2  = 2 * (height + width)\n",
    "    c2  = (1 - min_overlap) * width * height\n",
    "    sq2 = np.sqrt(b2 ** 2 - 4 * a2 * c2)\n",
    "    r2  = (b2 - sq2) / (2 * a2)\n",
    "\n",
    "    a3  = 4 * min_overlap\n",
    "    b3  = -2 * min_overlap * (height + width)\n",
    "    c3  = (min_overlap - 1) * width * height\n",
    "    sq3 = np.sqrt(b3 ** 2 - 4 * a3 * c3)\n",
    "    r3  = (b3 + sq3) / (2 * a3)\n",
    "    return min(r1, r2, r3)\n",
    "def gaussian2D(shape, sigma=1):\n",
    "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
    "    y, x = np.ogrid[-m:m+1,-n:n+1]\n",
    "\n",
    "    h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))\n",
    "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
    "    return h\n",
    "\n",
    "def draw_umich_gaussian(heatmap, center, radius, k=1):\n",
    "  diameter = 2 * radius + 1\n",
    "  gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
    "  # print(gaussian.shape)\n",
    "  x, y = int(center[0]), int(center[1])\n",
    "\n",
    "  height, width = heatmap.shape[0:2]\n",
    "    \n",
    "  left, right = min(x, radius), min(width - x, radius + 1)\n",
    "  top, bottom = min(y, radius), min(height - y, radius + 1)\n",
    "\n",
    "  masked_heatmap  = heatmap[y - top:y + bottom, x - left:x + right]\n",
    "  masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]\n",
    "  if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0: # TODO debug\n",
    "    np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n",
    "  return heatmap\n",
    "def draw_dense_reg(regmap, heatmap, center, value, radius, is_offset=False):\n",
    "  diameter = 2 * radius + 1\n",
    "  gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
    "  value = np.array(value, dtype=np.float32).reshape(-1, 1, 1)\n",
    "\n",
    "  dim = value.shape[0]\n",
    "  reg = np.ones((dim, diameter*2+1, diameter*2+1), dtype=np.float32) * value\n",
    "\n",
    "  if is_offset and dim == 2:\n",
    "    delta = np.arange(diameter*2+1) - radius\n",
    "    reg[0] = reg[0] - delta.reshape(1, -1)\n",
    "    reg[1] = reg[1] - delta.reshape(-1, 1)\n",
    "  \n",
    "  x, y = int(center[0]), int(center[1])\n",
    "\n",
    "  height, width = heatmap.shape[0:2]\n",
    "\n",
    "  left, right = min(x, radius), min(width - x, radius + 1)\n",
    "  top, bottom = min(y, radius), min(height - y, radius + 1)\n",
    "\n",
    "  masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n",
    "  masked_regmap = regmap[:, y - top:y + bottom, x - left:x + right]\n",
    "  masked_gaussian = gaussian[radius - top:radius + bottom,\n",
    "                             radius - left:radius + right]\n",
    "  masked_reg = reg[:, radius - top:radius + bottom,\n",
    "                      radius - left:radius + right]\n",
    "\n",
    "  if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0: # TODO debug\n",
    "    idx = (masked_gaussian >= masked_heatmap).reshape(\n",
    "      1, masked_gaussian.shape[0], masked_gaussian.shape[1])\n",
    "    masked_regmap = (1-idx) * masked_regmap + idx * masked_reg\n",
    "  regmap[:, y - top:y + bottom, x - left:x + right] = masked_regmap\n",
    "  return regmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "55ffd302-23cb-4ae5-963c-1748d08752da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44 76 76\n",
      "32 32\n",
      "[30 30]\n",
      "(22, 22)\n",
      "16 18 113 113\n",
      "97 95\n",
      "[32 32]\n",
      "(8, 9)\n",
      "38 38 70 70\n",
      "32 32\n",
      "[27 27]\n",
      "(19, 19)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hm = np.zeros((2,128,128))\n",
    "input_size = 128\n",
    "MODEL_SCALE=2\n",
    "fmap_dim = input_size//MODEL_SCALE\n",
    "hm = np.zeros((2,fmap_dim,fmap_dim))\n",
    "wh_regr = np.zeros((2, fmap_dim, fmap_dim), dtype=np.float32)\n",
    "regr = np.zeros((2, fmap_dim, fmap_dim), dtype=np.float32)\n",
    "# cs = np.array([[60,60,32,32],[64,64,96,96],[54,54,32,32]])\n",
    "cs = np.array([[44,44,76,76],[16,18,113,113],[38,38,70,70]])\n",
    "\n",
    "clses = np.array([0,1,0])\n",
    "# c = np.array([64,64,100,100])\n",
    "# print(cs.shape)\n",
    "\n",
    "hm_vis = hm.copy()\n",
    "for c,cl in zip(cs,clses):\n",
    "    x,y,x2,y2 = c\n",
    "    print(x,y,x2,y2)\n",
    "    w = int(x2-x)\n",
    "    h = int(y2-y)\n",
    "    print(w,h)\n",
    "    centers = [(x+x2)/2,(y+y2)/2]\n",
    "    # print(centers)\n",
    "    # print(h,w)\n",
    "    # h, w = h//MODEL_SCALE, w//MODEL_SCALE\n",
    "    # print(h,w)\n",
    "    c_s = np.array([int(i//MODEL_SCALE) for i in centers]).astype(np.int32)\n",
    "    print(c_s)\n",
    "    centers = np.array([(x+x2)/2,(y+y2)/2])\n",
    "    centers_int = centers.copy().astype(np.int32)\n",
    "    # print(centers-centers_int)\n",
    "    centers_sc = np.array([int(i)//MODEL_SCALE for i in centers])\n",
    "    # print(c_s)\n",
    "    radius = gaussian_radius2((math.ceil(h//MODEL_SCALE), math.ceil(w//MODEL_SCALE)),min_overlap=0.85)\n",
    "    radius = max(0, int(radius))\n",
    "    # print(\"radius:\", radius)\n",
    "    draw_umich_gaussian(hm[cl], c_s, \n",
    "                            radius)\n",
    "    draw_umich_gaussian(hm_vis[cl], c_s, \n",
    "                            radius)\n",
    "    draw_dense_reg(wh_regr,hm[cl],c_s,np.array([w/input_size,h/input_size]),radius)\n",
    "    draw_dense_reg(regr,hm[cl],c_s,centers-centers_int,radius)\n",
    "    # hm_vis*=255\n",
    "    print((c_s[0]-w//MODEL_SCALE//2,c_s[1]-h//MODEL_SCALE//2))\n",
    "    cv2.rectangle(hm_vis[cl],(c_s[0]-w//MODEL_SCALE//2,c_s[1]-h//MODEL_SCALE//2),(c_s[0]+w//MODEL_SCALE//2,c_s[1]+h//MODEL_SCALE//2),1)\n",
    "# plt.imshow(hm[0]*255,cmap='gray')\n",
    "# plt.show()\n",
    "# plt.imshow(hm[1]*255,cmap='gray')\n",
    "# plt.show()\n",
    "plt.imshow(hm_vis[0]+hm_vis[1],cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(wh_regr.transpose(1,2,0)[:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c256d108-18bf-40ad-9f16-f3e9f3d2ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wh_regr = np.zeros((2, fmap_dim, fmap_dim), dtype=np.float32)\n",
    "# regr = np.zeros((2, fmap_dim, fmap_dim), dtype=np.float32)\n",
    "# # regrs = boxes[:, 2:] - boxes[:, :2]  # width, height\n",
    "# # wh_regrs = cs[:, 2:].copy()/input_size # width, height\n",
    "# for c in cs:\n",
    "#     # print(r)\n",
    "#     # print(xc,yc,w,h)\n",
    "#     # w, h = abs(x2 - x1), abs(y2 - y1)\n",
    "#     # xc, yc = x1 + w // 2, y1 + h // 2\n",
    "#     x,y,x2,y2 = c\n",
    "#     w = (x2-x)/input_size\n",
    "#     h = (y2-y)/input_size\n",
    "#     print(w,h)\n",
    "#     centers = np.array([(x+x2)/2,(y+y2)/2])\n",
    "#     centers_int = centers.copy().astype(np.int32)\n",
    "#     print(centers-centers_int)\n",
    "#     centers_sc = np.array([int(i)//MODEL_SCALE for i in centers])\n",
    "#     scaled_xc = centers_sc[0]\n",
    "#     scaled_yc = centers_sc[1]\n",
    "#     # print(xc-scaled_xc, yc - scaled_yc)\n",
    "#     for i in range(-1, 1):\n",
    "#         for j in range(-1, 1):\n",
    "#             try:\n",
    "#                 a = max(scaled_xc + i, 0)\n",
    "#                 b = min(scaled_yc + j, fmap_dim)\n",
    "#                 # print(a,b,r)\n",
    "#                 wh_regr[:, a, b] = np.array([w,h])\n",
    "#                 regr[:,a,b] = centers-centers_int\n",
    "#                 print(\"wh_regr[:, {}, {}]={}\".format(a,b,wh_regr[:, a, b]))\n",
    "#                 print(\"regr[:, {}, {}]={}\".format(a,b,regr[:,a,b]))\n",
    "\n",
    "#             except Exception as e:  # noqa: E722\n",
    "#                 print(e)\n",
    "#                 pass\n",
    "# wh_regr[0] = wh_regr[0].T\n",
    "# wh_regr[1] = wh_regr[1].T\n",
    "# regr[0] = regr[0].T\n",
    "# regr[1] = regr[1].T\n",
    "# plt.imshow(wh_regr.transpose(1,2,0)[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "21baaf58-de4d-4b4b-b29f-e6e4e6517b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pred2box(heatmap, regr, threshold=0.5, scale=4, input_size=512):\n",
    "#     \"\"\"Convert model output to bounding boxes.\n",
    "#     Args:\n",
    "#         heatmap (np.ndarray): center heatmap, expected matrix with shapes [N, N].\n",
    "#         regr (np.ndarray): width and height coordinates, expected matrix with shapes [2, N, N]\n",
    "#         threshold (float): score threshold.\n",
    "#             If ``None`` then will be ignored filtering.\n",
    "#             Default is ``None``.\n",
    "#         scale (int): output scale, resulting coordinates will be multiplied by that constant.\n",
    "#             Default is ``4``.\n",
    "#         input_size (int): model input size.\n",
    "#             Default is ``512``.\n",
    "#     Returns:\n",
    "#         bounding boxes (np.ndarray with shape [M, 4]) and scores (np.ndarray with shape [M])\n",
    "#     \"\"\"\n",
    "#     cy, cx = torch.where(heatmap > threshold)\n",
    "#     print(cy,cx)\n",
    "#     boxes, scores = torch.empty((len(cy), 4), dtype=torch.int), torch.zeros(len(cx), dtype=torch.float)\n",
    "#     for i, (x, y) in enumerate(zip(cx, cy)):\n",
    "#         scores[i] = heatmap[y, x]\n",
    "#         print(scores[i],y,x)\n",
    "#         # x, y in segmentation scales -> upscale outputs\n",
    "#         sx, sy = int(x * scale), int(y * scale)\n",
    "#         w, h = (regr[:, y, x] * input_size)\n",
    "#         print(w,h)\n",
    "#         boxes[i, 0] = sx - w / 2\n",
    "#         boxes[i, 1] = sy - h / 2\n",
    "#         boxes[i, 2] = sx + w / 2\n",
    "#         boxes[i, 3] = sy + h / 2\n",
    "#     # boxes = torch.Tensor(np.array(boxes))\n",
    "#     # scores = torch.Tensor(np.array(scores))\n",
    "#     box_cxcywh = box_convert(boxes, in_fmt=\"xyxy\", out_fmt=\"cxcywh\")\n",
    "#     # boxes[:,2:] = boxes[:,2:] - boxes[:,:2]\n",
    "#     print(boxes.dtype)\n",
    "#     idx = nms(boxes, scores, 0.5)\n",
    "#     # boxes = boxes[idx]\n",
    "#     # scores = scores[idx]\n",
    "#     return box_cxcywh, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7736857e-9d1b-4739-b4be-af4c94a633fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hm_vis_2 = hm.copy()\n",
    "# boxes, scores = pred2box(torch.from_numpy(hm),torch.from_numpy(wh_regr),threshold=0.99, scale=1,input_size=128)\n",
    "# for ind,(b,s) in enumerate(zip(boxes, scores)):\n",
    "#     cx,cy,w,h = [int(i) for i in b.numpy()]\n",
    "#     print(cx,cy,w,h)\n",
    "#     cv2.rectangle(hm_vis_2, (cx-w//2,cy-h//2),(cx+w//2,cy+h//2),1)\n",
    "# plt.imshow(hm_vis_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0078fd7c-9376-42e5-a5e6-c9037bf0b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 16.,  17., 113., 112.],\n",
      "        [ 38.,  38.,  70.,  70.],\n",
      "        [ 44.,  44.,  76.,  76.]]) tensor([1., 1., 1.]) tensor([1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def nonempty(boxes,threshold: float = 0.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Find boxes that are non-empty.\n",
    "        A box is considered empty, if either of its side is no larger than threshold.\n",
    "        Returns:\n",
    "            Tensor:\n",
    "                a binary vector which represents whether each box is empty\n",
    "                (False) or non-empty (True).\n",
    "        \"\"\"\n",
    "        widths = boxes[:, 2] - boxes[:, 0]\n",
    "        heights = boxes[:, 3] - boxes[:, 1]\n",
    "        keep = (widths > threshold) & (heights > threshold)\n",
    "        return keep\n",
    "\n",
    "def _gather_feature(feat, ind, mask=None):\n",
    "  dim = feat.size(2)\n",
    "  ind = ind.unsqueeze(2).expand(ind.size(0), ind.size(1), dim)\n",
    "  feat = feat.gather(1, ind)\n",
    "  if mask is not None:\n",
    "    mask = mask.unsqueeze(2).expand_as(feat)\n",
    "    feat = feat[mask]\n",
    "    feat = feat.view(-1, dim)\n",
    "  return feat\n",
    "\n",
    "\n",
    "def _tranpose_and_gather_feature(feat, ind):\n",
    "  feat = feat.permute(0, 2, 3, 1).contiguous()\n",
    "  feat = feat.view(feat.size(0), -1, feat.size(3))\n",
    "  feat = _gather_feature(feat, ind)\n",
    "  return feat\n",
    "\n",
    "def _topk(scores, K=40):\n",
    "  batch, cat, height, width = scores.size()\n",
    "\n",
    "  topk_scores, topk_inds = torch.topk(scores.view(batch, cat, -1), K)\n",
    "\n",
    "  topk_inds = topk_inds % (height * width)\n",
    "  topk_ys = (topk_inds / width).int().float()\n",
    "  topk_xs = (topk_inds % width).int().float()\n",
    "\n",
    "  topk_score, topk_ind = torch.topk(topk_scores.view(batch, -1), K)\n",
    "  topk_clses = (topk_ind / K).int()\n",
    "  topk_inds = _gather_feature(topk_inds.view(batch, -1, 1), topk_ind).view(batch, K)\n",
    "  topk_ys = _gather_feature(topk_ys.view(batch, -1, 1), topk_ind).view(batch, K)\n",
    "  topk_xs = _gather_feature(topk_xs.view(batch, -1, 1), topk_ind).view(batch, K)\n",
    "\n",
    "  return topk_score, topk_inds, topk_clses, topk_ys, topk_xs\n",
    "\n",
    "def decode_predictions(hm,regr,wh_regr,MODEL_SCALE=None,K=100):\n",
    "    '''\n",
    "    hm: BxCxHxW\n",
    "    regr: Bx2xHxW\n",
    "    wh_regr: Bx2xHxW\n",
    "    '''\n",
    "    \n",
    "    batch,cat, height, width = hm.shape\n",
    "\n",
    "    scores, inds, clses, ys, xs = _topk(hm, K=K)\n",
    "\n",
    "    regs = _tranpose_and_gather_feature(regr, inds)\n",
    "    regs = regs.view(batch, K, 2)\n",
    "\n",
    "    xs = xs.view(batch, K, 1)*MODEL_SCALE + regs[:, :, 0:1]\n",
    "    ys = ys.view(batch, K, 1)*MODEL_SCALE + regs[:, :, 1:2]\n",
    "\n",
    "\n",
    "\n",
    "    w_h_ = _tranpose_and_gather_feature(wh_regr, inds)\n",
    "    w_h_ = w_h_.view(batch, K, 2)*input_size\n",
    "\n",
    "    clses = clses.view(batch, K, 1).float().squeeze(0).squeeze(-1)\n",
    "    scores = scores.view(batch, K, 1).squeeze(0).squeeze(-1).type(torch.float32)\n",
    "    boxes = torch.cat([xs - w_h_[..., 0:1] / 2,\n",
    "                      ys - w_h_[..., 1:2] / 2,\n",
    "                      xs + w_h_[..., 0:1] / 2,\n",
    "                      ys + w_h_[..., 1:2] / 2], dim=2).squeeze(0)\n",
    "\n",
    "    keep = nonempty(boxes)\n",
    "    boxes=boxes[keep]\n",
    "    scores=scores[keep]\n",
    "    clses = clses[keep]\n",
    "    idx = nms(boxes, scores, 0.5)\n",
    "    boxes = boxes[idx]\n",
    "    scores = scores[idx]\n",
    "    clses = clses[idx]\n",
    "    \n",
    "    return boxes,scores,clses\n",
    "\n",
    "hm_b = torch.from_numpy(hm).unsqueeze(0)\n",
    "regr_b = torch.from_numpy(regr).unsqueeze(0)\n",
    "wh_regr_b = torch.from_numpy(wh_regr).unsqueeze(0)\n",
    "boxes,scores,clses = decode_predictions(hm_b,regr_b,wh_regr_b,MODEL_SCALE=MODEL_SCALE,K=100)\n",
    "print(boxes,scores,clses)\n",
    "# if want coco representation: \n",
    "# box_cxcywh = box_convert(boxes, in_fmt=\"xyxy\", out_fmt=\"cxcywh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff54b7-a548-4b07-ae93-3f805c432c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe6562b2-b6c8-4b1f-a5c9-4c65b3198874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def process_centernet_output(\n",
    "#     predicted_heatmap,  # logits\n",
    "#     predicted_regression,\n",
    "#     gt_boxes,\n",
    "#     gt_labels,\n",
    "#     confidence_threshold=0.5,\n",
    "#     iou_threshold=0.5,\n",
    "# ):\n",
    "#     \"\"\"Generate bbox and classes from CenterNet model outputs.\n",
    "#     Args:\n",
    "#         predicted_heatmap (torch.Tensor): predicted center heatmap logits,\n",
    "#             expected shapes [batch, height, width, num classes].\n",
    "#         predicted_regression (torch.Tensor): predicted HW regression,\n",
    "#             expected shapes [batch, height, width, 2].\n",
    "#         gt_boxes (List[torch.Tensor]): list with sample bounding boxes.\n",
    "#         gt_labels (List[torch.Tensor]): list with sample bounding box labels.\n",
    "#         confidence_threshold (float): confidence threshold,\n",
    "#             proposals with lover values than threshold will be ignored.\n",
    "#             Default is ``0.5``.\n",
    "#         iou_threshold (float): IoU threshold to use in NMS.\n",
    "#             Default is ``0.5``.\n",
    "#     Yields:\n",
    "#         predicted sample (np.ndarray) and ground truth sample (np.ndarray)\n",
    "#     \"\"\"\n",
    "#     batch_size = predicted_heatmap.size(0)\n",
    "\n",
    "#     hm = predicted_heatmap.sigmoid()\n",
    "#     pooled = F.max_pool2d(hm, kernel_size=(3, 3), stride=1, padding=1)\n",
    "#     hm *= torch.logical_and(\n",
    "#         hm >= confidence_threshold, pooled >= confidence_threshold\n",
    "#     ).float()\n",
    "\n",
    "#     hm_numpy = hm.detach().cpu().numpy()\n",
    "#     reg_numpy = predicted_regression.detach().cpu().numpy()\n",
    "\n",
    "#     for i in range(batch_size):\n",
    "#         sample_boxes = []\n",
    "#         sample_classes = []\n",
    "#         sample_scores = []\n",
    "#         for cls_idx in range(hm_numpy.shape[1]):\n",
    "#             # build predictions\n",
    "#             cls_boxes, cls_scores = pred2box(\n",
    "#                 hm_numpy[i, cls_idx], reg_numpy[i], threshold=0, scale=4, input_size=512\n",
    "#             )\n",
    "\n",
    "#             # skip empty label predictions\n",
    "#             if cls_scores.shape[0] == 0:\n",
    "#                 continue\n",
    "\n",
    "#             cls_boxes = cls_boxes / 512.0\n",
    "\n",
    "#             cls_boxes, cls_classes, cls_scores = nms_filter(\n",
    "#                 cls_boxes,\n",
    "#                 np.full(len(cls_scores), cls_idx),\n",
    "#                 cls_scores,\n",
    "#                 iou_threshold=iou_threshold,\n",
    "#             )\n",
    "#             sample_boxes.append(cls_boxes)\n",
    "#             sample_classes.append(cls_classes)\n",
    "#             sample_scores.append(cls_scores)\n",
    "#         # skip empty predictions\n",
    "#         if len(sample_boxes) == 0:\n",
    "#             continue\n",
    "\n",
    "#         sample_boxes = np.concatenate(sample_boxes, 0)\n",
    "#         sample_classes = np.concatenate(sample_classes, 0)\n",
    "#         sample_scores = np.concatenate(sample_scores, 0)\n",
    "\n",
    "#         pred_sample = np.concatenate(\n",
    "#             [sample_boxes, sample_classes[:, None], sample_scores[:, None]], -1\n",
    "#         )\n",
    "#         pred_sample = pred_sample.astype(np.float32)\n",
    "\n",
    "#         sample_gt_bboxes = gt_boxes[i].detach().cpu()\n",
    "#         sample_gt_classes = gt_labels[i].detach().cpu()\n",
    "#         gt_sample = np.zeros((sample_gt_classes.shape[0], 7), dtype=np.float32)\n",
    "#         gt_sample[:, :4] = sample_gt_bboxes\n",
    "#         gt_sample[:, 4] = sample_gt_classes\n",
    "\n",
    "#         yield pred_sample, gt_sample\n",
    "# class CenterNetDataset(Dataset):\n",
    "#     def __init__(self, coco_json_path, images_dir=None, transforms=None, down_ratio=4):\n",
    "#         self.file = coco_json_path\n",
    "#         self.img_dir = images_dir\n",
    "#         self.transforms = transforms\n",
    "#         self.down_ratio = down_ratio\n",
    "\n",
    "#         self.images, self.categories = load_coco_json(coco_json_path)\n",
    "#         self.images_list = sorted(self.images.keys())\n",
    "\n",
    "#         self.class_to_cid = {\n",
    "#             cls_idx: cat_id\n",
    "#             for cls_idx, cat_id in enumerate(sorted(self.categories.keys()))\n",
    "#         }\n",
    "#         self.cid_to_class = {v: k for k, v in self.class_to_cid.items()}\n",
    "#         self.num_classes = len(self.class_to_cid)\n",
    "#         self.class_labels = [\n",
    "#             self.categories[self.class_to_cid[cls_idx]]\n",
    "#             for cls_idx in range(len(self.class_to_cid))\n",
    "#         ]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images_list)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         img_id = self.images_list[index]\n",
    "#         img_record = self.images[img_id]\n",
    "\n",
    "#         path = img_record[\"file_name\"]\n",
    "#         if self.img_dir is not None:\n",
    "#             path = os.path.join(self.img_dir, path)\n",
    "#         image = read_image(path)\n",
    "#         original_size = [image.shape[0], image.shape[1]]  # height, width\n",
    "\n",
    "#         boxes = []  # each element is a tuple of (x1, y1, x2, y2, \"class\")\n",
    "#         for annotation in img_record[\"annotations\"]:\n",
    "#             pixel_xywh = annotation[\"bbox\"]\n",
    "#             # skip bounding boxes with 0 height or 0 width\n",
    "#             if pixel_xywh[2] == 0 or pixel_xywh[3] == 0:\n",
    "#                 continue\n",
    "#             xyxy = pixels_to_absolute(\n",
    "#                 pixel_xywh, width=img_record[\"width\"], height=img_record[\"height\"]\n",
    "#             )\n",
    "#             xyxy = clip(xyxy, 0.0, 1.0)\n",
    "#             bbox_class = str(self.cid_to_class[annotation[\"category_id\"]])\n",
    "#             boxes.append(xyxy + [str(bbox_class)])\n",
    "\n",
    "#         if self.transforms is not None:\n",
    "#             transformed = self.transforms(image=image, bboxes=boxes)\n",
    "#             image, boxes = transformed[\"image\"], transformed[\"bboxes\"]\n",
    "#         else:\n",
    "#             image = torch.from_numpy((image / 255.0).astype(np.float32)).permute(2, 0, 1)\n",
    "\n",
    "#         labels = np.array([int(items[4]) for items in boxes])\n",
    "#         boxes = np.array([items[:4] for items in boxes], dtype=np.float32)\n",
    "#         # boxes = change_box_order(boxes, \"xyxy2xywh\")  # (x1, y1, x2, y2) -> (cx, cy, w, h)\n",
    "\n",
    "#         heatmap_height = image.shape[1] // self.down_ratio\n",
    "#         heatmap_width = image.shape[2] // self.down_ratio\n",
    "#         # draw class centers\n",
    "#         heatmap = np.zeros(\n",
    "#             (self.num_classes, heatmap_height, heatmap_width), dtype=np.float32\n",
    "#         )\n",
    "#         for (x1, y1, x2, y2), cls_channel in zip(boxes, labels):\n",
    "#             w, h = abs(x2 - x1), abs(y2 - y1)\n",
    "#             xc, yc = x1 + w // 2, y1 + h // 2\n",
    "#             scaled_xc = int(xc * heatmap_width)\n",
    "#             scaled_yc = int(yc * heatmap_height)\n",
    "#             draw_msra_gaussian(\n",
    "#                 heatmap, cls_channel, (scaled_xc, scaled_yc), sigma=np.clip(w * h, 2, 4)\n",
    "#             )\n",
    "#         # draw regression squares\n",
    "#         wh_regr = np.zeros((2, heatmap_height, heatmap_width), dtype=np.float32)\n",
    "#         regrs = boxes[:, 2:] - boxes[:, :2]  # width, height\n",
    "#         for r, (x1, y1, x2, y2) in zip(regrs, boxes):\n",
    "#             w, h = abs(x2 - x1), abs(y2 - y1)\n",
    "#             xc, yc = x1 + w // 2, y1 + h // 2\n",
    "#             scaled_xc = int(xc * heatmap_width)\n",
    "#             scaled_yc = int(yc * heatmap_height)\n",
    "#             for i in range(-2, 2 + 1):\n",
    "#                 for j in range(-2, 2 + 1):\n",
    "#                     try:\n",
    "#                         a = max(scaled_xc + i, 0)\n",
    "#                         b = min(scaled_yc + j, heatmap_height)\n",
    "#                         wh_regr[:, a, b] = r\n",
    "#                     except:  # noqa: E722\n",
    "#                         pass\n",
    "#         wh_regr[0] = wh_regr[0].T\n",
    "#         wh_regr[1] = wh_regr[1].T\n",
    "\n",
    "#         return {\n",
    "#             \"image\": image,\n",
    "#             \"original_size\": original_size,\n",
    "#             \"size\": [image.size(1), image.size(2)],\n",
    "#             \"heatmap\": torch.from_numpy(heatmap),\n",
    "#             \"wh_regr\": torch.from_numpy(wh_regr),\n",
    "#             \"bboxes\": boxes,\n",
    "#             \"labels\": labels,\n",
    "#         }\n",
    "\n",
    "#     @staticmethod\n",
    "#     def collate_fn(batch):\n",
    "#         keys = list(batch[0].keys())\n",
    "#         packed_batch = {k: [] for k in keys}\n",
    "#         for element in batch:\n",
    "#             for k in keys:\n",
    "#                 packed_batch[k].append(element[k])\n",
    "#         for k in (\"image\", \"heatmap\", \"wh_regr\"):\n",
    "#             packed_batch[k] = torch.stack(packed_batch[k], 0)\n",
    "#         return packed_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c213168-155f-4d89-a342-f5f323f04f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
