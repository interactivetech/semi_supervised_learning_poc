{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69997c47-d184-470f-8fad-2792f9501b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0970f9-f0e7-4be6-93ac-81fc1fa0d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/usb/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import wrn_28_2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from ema import EMA,EMADriver, set_ema_model\n",
    "from train import trainer\n",
    "from eval import predict, eval\n",
    "from tqdm import tqdm\n",
    "from semilearn import get_dataset, get_data_loader, get_net_builder, get_algorithm, get_config, Trainer\n",
    "from semilearn.datasets.cv_datasets import get_cifar\n",
    "import argparse\n",
    "from semilearn.core.utils import get_dataset, get_data_loader, get_optimizer, get_cosine_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "from flexmatch import FlexMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e531f8-af43-432e-b905-b4a65c005dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "lb count: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "ulb count: [15, 15, 15, 15, 15, 15, 15, 15, 15, 15]\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "m = wrn_28_2(pretrained=False,pretrained_path=None ,num_classes=10)\n",
    "_=m.to(device)\n",
    "ema = wrn_28_2(pretrained=False,pretrained_path=None ,num_classes=10)\n",
    "ema = set_ema_model(ema, m)\n",
    "emaA = EMADriver(model=m,ema_model=ema,ema_m=0.999)\n",
    "# emaA = EMADriver(model=m,ema_model=ema,ema_m=0.999)\n",
    "emaA.before_run()\n",
    "args_d = {'dataset': 'cifar10',\n",
    "         'num_classes': 10,\n",
    "         'train_sampler': 'RandomSampler',\n",
    "         'num_workers': 8,\n",
    "         'lb_imb_ratio': 1,\n",
    "         'ulb_imb_ratio':1.0,\n",
    "          'batch_size': 32,\n",
    "         'ulb_num_labels': 150,\n",
    "         'img_size': 32,\n",
    "         'crop_ratio': 0.875,\n",
    "         'num_labels': 30,\n",
    "         'seed': 1,\n",
    "         'epoch': 3,\n",
    "         'num_train_iter':150,\n",
    "         'net': 'wrn_28_8',\n",
    "         'optim': 'SGD',\n",
    "         'lr': 0.03,\n",
    "         'momentum': 0.9,\n",
    "         'weight_decay': 0.0005,\n",
    "         'layer_decay': 0.75,\n",
    "          'num_warmup_iter': 0,\n",
    "         'algorithm': 'fixmatch',\n",
    "         'data_dir': './data',\n",
    "         'uratio': 3,\n",
    "         'eval_batch_size': 64}\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Semi-Supervised Learning (USB semilearn package)')\n",
    "args = parser.parse_args(\"\")\n",
    "# args\n",
    "for k in args_d:\n",
    "        setattr(args, k, args_d[k])\n",
    "# lb_dset, ulb_dset, eval_dset = get_cifar(args,\n",
    "#           alg=None, \n",
    "#           name='cifar10',\n",
    "#           num_labels=4000,\n",
    "#           num_classes=10,\n",
    "#           data_dir='./data',\n",
    "#           include_lb_to_ulb=True)\n",
    "dataset_dict = get_dataset(args, \n",
    "                           args.algorithm, \n",
    "                           args.dataset, \n",
    "                           args.num_labels, \n",
    "                           args.num_classes, \n",
    "                           data_dir=args.data_dir,\n",
    "                          include_lb_to_ulb=False)\n",
    "train_lb_loader = get_data_loader(args, dataset_dict['train_lb'], args.batch_size)\n",
    "train_lb_loader = get_data_loader(args, dataset_dict['train_lb'], args.batch_size)\n",
    "train_ulb_loader = get_data_loader(args, dataset_dict['train_ulb'], int(args.batch_size * args.uratio))\n",
    "eval_loader = get_data_loader(args, dataset_dict['eval'], args.eval_batch_size)\n",
    "\n",
    "optimizer = get_optimizer(m, args.optim, args.lr, args.momentum, args.weight_decay, args.layer_decay)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                            args.num_train_iter,\n",
    "                                            num_warmup_steps=args.num_warmup_iter)\n",
    "loss_ce = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c95d575-3f8b-4d3b-8f8b-716a4f2c34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = FlexMatch(T, \n",
    "#              p_cutoff, \n",
    "#              ulb_dest_len=len(dataset_dict['train_ulb']),\n",
    "#              num_classes=10,\n",
    "#              model=m)\n",
    "\n",
    "# m = wrn_28_2(pretrained=False,pretrained_path=None ,num_classes=10)\n",
    "# _=m.to(device)\n",
    "# ema = wrn_28_2(pretrained=False,pretrained_path=None ,num_classes=10)\n",
    "# ema = set_ema_model(ema, m)\n",
    "# emaA = EMADriver(model=m,ema_model=ema,ema_m=0.999)\n",
    "# emaA = EMADriver(model=m,ema_model=ema,ema_m=0.999)\n",
    "# emaA.before_run()\n",
    "\n",
    "f = FlexMatch(T=1.0, \n",
    "             p_cutoff=0.95, \n",
    "             ulb_dest_len=len(dataset_dict['train_ulb']),\n",
    "             num_classes=10,\n",
    "             model=m,\n",
    "             ema_model=ema,\n",
    "             loss_ce=loss_ce,\n",
    "             scheduler=scheduler,\n",
    "             optimizer=optimizer,\n",
    "             device=device,\n",
    "             train_lb_loader=train_lb_loader,\n",
    "             train_ulb_loader=train_ulb_loader,\n",
    "             ulb_loss_ratio=1.0,\n",
    "             hard_label=True, \n",
    "             thresh_warmup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c5b008-e73d-41ce-ade3-ccc99b1e9fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 202/1000 [1:32:14<6:04:22, 27.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m steps, sup_loss,unsup_loss,total_loss, mask_ratio \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mfit(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m/run/determined/workdir/semi_supervised_learning_poc/flexmatch.py:252\u001b[0m, in \u001b[0;36mFlexMatch.fit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    250\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ind,(data_lb, data_ulb) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_lb_loader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_ulb_loader)):\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;66;03m# print(data_lb.keys())\u001b[39;00m\n\u001b[1;32m    254\u001b[0m         idx_lb \u001b[38;5;241m=\u001b[39m data_lb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx_lb\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    255\u001b[0m         x_lb \u001b[38;5;241m=\u001b[39m data_lb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_lb\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/envs/usb/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/usb/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1186\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1186\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/usb/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1152\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1154\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/envs/usb/lib/python3.8/site-packages/torch/utils/data/dataloader.py:990\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 990\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m    995\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/usb/lib/python3.8/multiprocessing/queues.py:116\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/usb/lib/python3.8/site-packages/torch/multiprocessing/reductions.py:289\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 289\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m/opt/conda/envs/usb/lib/python3.8/multiprocessing/resource_sharer.py:58\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _resource_sharer\u001b[38;5;241m.\u001b[39mget_connection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id) \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/usb/lib/python3.8/multiprocessing/reduction.py:189\u001b[0m, in \u001b[0;36mrecv_handle\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m'''Receive a handle over a local connection.'''\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mfromfd(conn\u001b[38;5;241m.\u001b[39mfileno(), socket\u001b[38;5;241m.\u001b[39mAF_UNIX, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecvfds\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/usb/lib/python3.8/multiprocessing/reduction.py:157\u001b[0m, in \u001b[0;36mrecvfds\u001b[0;34m(sock, size)\u001b[0m\n\u001b[1;32m    155\u001b[0m a \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    156\u001b[0m bytes_size \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m*\u001b[39m size\n\u001b[0;32m--> 157\u001b[0m msg, ancdata, flags, addr \u001b[38;5;241m=\u001b[39m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecvmsg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCMSG_SPACE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m msg \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ancdata:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps, sup_loss,unsup_loss,total_loss, mask_ratio = f.fit(epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866f6c3-8dd2-409e-b8bc-6d182e271989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14daec81-3416-48c9-be18-7641c38548b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps,sup_loss)\n",
    "plt.plot(steps,unsup_loss)\n",
    "plt.plot(steps,total_loss)\n",
    "plt.plot(steps,mask_ratio)\n",
    "#mask_ratio\n",
    "plt.legend(['sup_loss','unsup_loss','total_loss','mask_ratio'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0de4e3-dc14-4941-8a8b-e45984a16a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1,balanced_top1, precision, recall, F1, cf_mat  = eval(m,emaA.ema,eval_loader,device,return_gt=True,use_ema_model=False)\n",
    "no_ema = {\n",
    "    'top1':top1,\n",
    "    'balanced_top1':balanced_top1,\n",
    "    'precision':precision,\n",
    "    'recall':recall,\n",
    "    'F1':F1,\n",
    "    'cf_mat':cf_mat\n",
    "}\n",
    "top1,balanced_top1, precision, recall, F1, cf_mat  = eval(m,emaA.ema,eval_loader,device,return_gt=True,use_ema_model=True)\n",
    "ema_res = {\n",
    "    'top1':top1,\n",
    "    'balanced_top1':balanced_top1,\n",
    "    'precision':precision,\n",
    "    'recall':recall,\n",
    "    'F1':F1,\n",
    "    'cf_mat':cf_mat\n",
    "}\n",
    "for k in ema_res.keys():\n",
    "    if k!='cf_mat':\n",
    "        print(k,no_ema[k],ema_res[k])\n",
    "        plt.bar(['no_ema','ema'],[no_ema[k],ema_res[k]])\n",
    "        plt.title(\"{} No EMA vs EMA\".format(k))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e55045-681b-49ed-9262-df7d1b77a765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5937ed-f756-4cea-ac2e-1f166e6bdf56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usb",
   "language": "python",
   "name": "usb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
